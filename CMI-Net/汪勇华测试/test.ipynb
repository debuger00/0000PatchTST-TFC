{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])\n"
     ]
    }
   ],
   "source": [
    "# 示例输入：Batch=2, Input length=3, Channel=4\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "])\n",
    "\n",
    "x_flatten = x.view(2, -1)\n",
    "print(x_flatten)\n",
    "# 输出：\n",
    "# tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
    "#         [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])\n"
     ]
    }
   ],
   "source": [
    "# 示例输入：Batch=2, Input length=3, Channel=4\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "])\n",
    "\n",
    "x_flatten = x.view(2, 12)\n",
    "print(x_flatten)\n",
    "# 输出：\n",
    "# tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
    "#         [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  5,  9,  2,  6, 10,  3,  7, 11,  4,  8, 12],\n",
      "        [13, 17, 21, 14, 18, 22, 15, 19, 23, 16, 20, 24]])\n"
     ]
    }
   ],
   "source": [
    "# 示例输入：Batch=2, Input length=3, Channel=4\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "])\n",
    "\n",
    "# 首先调整维度顺序，将input length维度放到最后\n",
    "x_permuted = x.permute(0, 2, 1)  # 变成 [batch, channel, length]\n",
    "x_flatten = x_permuted.reshape(2, -1)\n",
    "print(x_flatten)\n",
    "# 输出：\n",
    "# tensor([[ 1,  5,  9,  2,  6, 10,  3,  7, 11,  4,  8, 12],\n",
    "#        [13, 17, 21, 14, 18, 22, 15, 19, 23, 16, 20, 24]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  5,  9,  2,  6, 10,  3,  7, 11,  4,  8, 12],\n",
      "        [13, 17, 21, 14, 18, 22, 15, 19, 23, 16, 20, 24]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "])\n",
    "x = x.permute(0,2,1) # x: [Batch, Channel, Input length]\n",
    "x = x.reshape(x.size(0), -1)# x: [Batch, Channel * Input length]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 使用自适应平均池化\u001b[39;00m\n\u001b[0;32m      8\u001b[0m adaptive_pool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d((\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43madaptive_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 输出形状：[32, 128, 1, 1]\u001b[39;00m\n",
      "File \u001b[1;32me:\\aaa_SpecializedSoftware\\MiniConda\\envs\\pytorch_1_12_1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\aaa_SpecializedSoftware\\MiniConda\\envs\\pytorch_1_12_1\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:1179\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\aaa_SpecializedSoftware\\MiniConda\\envs\\pytorch_1_12_1\\lib\\site-packages\\torch\\nn\\functional.py:1214\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[0;32m   1213\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_output_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "## 示例输入：Batch=2, Input length=3, Channel=4\n",
    "import torch.nn as nn\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "])\n",
    "# 使用自适应平均池化\n",
    "adaptive_pool = nn.AdaptiveAvgPool2d((1))\n",
    "output = adaptive_pool(x)  # 输出形状：[32, 128, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用AdaptiveAvgPool1d的输出:\n",
      "输出形状: torch.Size([2, 4, 1])\n",
      "tensor([[[ 5.],\n",
      "         [ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.]],\n",
      "\n",
      "        [[17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]]])\n"
     ]
    }
   ],
   "source": [
    "# 示例输入：Batch=2, Input length=3, Channel=4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "], dtype=torch.float32)  # 注意：需要使用float类型\n",
    "\n",
    "# # 方法1：直接使用mean\n",
    "# output = x.mean(dim=-1)  # 在最后一个维度（channel维度）上取平均\n",
    "# print(\"输出形状:\", output.shape)  # 输出形状: torch.Size([2, 3])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# 方法2：使用AdaptiveAvgPool1d\n",
    "adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "# 需要先调整维度顺序，将channel维度放到第二维\n",
    "x_permuted = x.permute(0, 2, 1)  # [2, 4, 3]\n",
    "output2 = adaptive_pool(x_permuted).squeeze(1)  # squeeze去掉长度为1的维度\n",
    "print(\"\\n使用AdaptiveAvgPool1d的输出:\")\n",
    "print(\"输出形状:\", output2.shape)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.5000]],\n",
      "\n",
      "        [[18.5000]]])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# 示例输入：Batch=2, Input length=3, Channel=4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
    "], dtype=torch.float32)  # 注意：需要使用float类型\n",
    "\n",
    "x = avg_pool(x)\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1_12_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
